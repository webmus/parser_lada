# Рекомендации по устранению дублей и ускорению публикации

## 1. Параллельный запуск action=news
В сценарии `action=news` отсутствует любая синхронизация. Процесс одновременно читает весь `list.dat`,
выполняет модули парсинга и только после завершения цикла перезаписывает файл, поэтому два параллельных
запуска обрабатывают одну и ту же запись как «Новый».【F:parsing-lada.xml†L4406-L4501】

**Что сделать**
- Добавить файловую блокировку (`flock`) или иной механизм эксклюзивного запуска по аналогии с `action=list`.
- При чтении очереди помечать элемент как «В работе» и сразу сохранять файл, чтобы второй процесс увидел
  изменённый статус.
- Оборачивать работу с `list.dat` в `flock` или перезаписать очередь на хранение в БД.

Это остановит конкурентное выполнение модулей и устранит повторный запуск рерайта/публикации для одной и той же новости.

## 2. Защита на уровне базы данных
Даже если очередь будет защищена от гонок, в `addPost()` остаётся окно между `SELECT` и `INSERT`, поэтому
два процесса успевают вставить два идентичных ряда с одинаковым `url`, если стартовали параллельно.【F:parsing-lada.xml†L4896-L5074】

**Что сделать**
- Создать уникальный индекс на поле `url` таблицы `{prefix}_post` и/или использовать `INSERT ... ON DUPLICATE KEY UPDATE`.
- Рассмотреть перевод проверки «существует ли запись» на `INSERT IGNORE` или `REPLACE` внутри транзакции.

Такой барьер исключит дубль даже при ошибке синхронизации в очереди.

## 3. Влияние расширенного логирования на время публикации
Каждый вызов `addLog()` пишет строку в файл с `LOCK_EX`, а `doGPT()` сохраняет полный JSON-ответ модели в лог.
Чем длиннее ответ, тем больше файловых операций и ожиданий блокировок при последовательной обработке нескольких
полей (full, short, title, meta).【F:parsing-lada.xml†L4561-L4634】【F:parsing-lada.xml†L4660-L4744】

**Что сделать**
- Оставить подробное логирование только на период диагностики (например, обрезать ответ до N символов или вынести
  отладку в отдельный канал).
- Проверить, не пишется ли одновременно `ROOT_DIR/../log.txt`, чтобы исключить контенцию нескольких файловых логов.

После устранения дублей время публикации сократится естественно, но уменьшение объёма логов дополнительно снизит задержки
на файловых операциях.

## 4. Дополнительные наблюдения
- `addToList()` тоже работает без блокировок и перезаписывает весь `list.dat`, поэтому при одновременном сборе ссылок из нескольких
  cron-задач возможны потери элементов. Стоит синхронизировать и этот участок.【F:parsing-lada.xml†L4561-L4593】
- Рассмотреть перенос очереди в таблицу БД — это упростит постановку статусов («Новый», «В работе», «Готово») и сделает транзакционные
  проверки естественными.

Внедрение указанных механизмов позволит убрать дубли публикаций и стабилизировать время обработки без изменений логики рерайта.

## 5. Ошибка `Table 'dle_storage' was not locked with LOCK TABLES`
- Ветка публикации блокирует только `{prefix}_post` через `LOCK TABLES`, но после этого тот же коннект продолжает работать с другими
  таблицами DLE, например читает `dle_storage` при инициализации шаблонов. MySQL запрещает любые запросы к таблицам, которые не включены
  в список блокировки, поэтому появляется ошибка 1100.【F:parsing-lada.xml†L5067-L5085】

**Что сделать**
- Свести блокировки к транзакции: заменить `LOCK TABLES ... WRITE`/`UNLOCK TABLES` на `START TRANSACTION` + `SELECT ... FOR UPDATE` и
  `COMMIT`, чтобы исключить конфликт с автозапросами DLE.
- Если блокировка всё же нужна, перечислить в `LOCK TABLES` полный набор таблиц, которые могут использоваться во время цикла
  публикации (включая `dle_storage`), а затем обязательно вызывать `UNLOCK TABLES` сразу после критичного участка.
- Проверить, что в момент генерации ответа не вызывается `addLog()` или другие функции, обращающиеся к БД до `UNLOCK TABLES` — их стоит
  вынести после разблокировки или перевести на файловое логирование без SQL.

Такой пересмотр блокировок устранит ошибку `Table 'dle_storage' was not locked with LOCK TABLES` и позволит плагину корректно
выполнять фоновые запросы DLE.
